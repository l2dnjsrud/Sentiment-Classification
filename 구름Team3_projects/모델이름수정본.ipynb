{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"모델이름수정본.ipynb","provenance":[],"collapsed_sections":["GjK9vX7GAVMP"]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Import requirements"],"metadata":{"id":"WJN5a_UHAVL_"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:31:35.625973Z","iopub.execute_input":"2021-10-31T12:31:35.627347Z","iopub.status.idle":"2021-10-31T12:31:46.094639Z","shell.execute_reply.started":"2021-10-31T12:31:35.627144Z","shell.execute_reply":"2021-10-31T12:31:46.09357Z"},"trusted":true,"id":"Cw-zZ-E9AVMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pdb\n","import wandb\n","import argparse\n","from dataclasses import dataclass, field\n","from typing import Optional\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence\n","\n","import numpy as np\n","from tqdm import tqdm, trange\n","from transformers.optimization import Adafactor\n","\n","from transformers import (\n","    BertForSequenceClassification,\n","    BertTokenizer,\n","    AutoConfig,\n","    AdamW,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer\n",")"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:31:46.098148Z","iopub.execute_input":"2021-10-31T12:31:46.09873Z","iopub.status.idle":"2021-10-31T12:31:53.789059Z","shell.execute_reply.started":"2021-10-31T12:31:46.098684Z","shell.execute_reply":"2021-10-31T12:31:53.788067Z"},"trusted":true,"id":"ouTvp_oJAVMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Preprocess"],"metadata":{"id":"4vBU6AkiAVMD"}},{"cell_type":"code","source":["def make_id_file(task, tokenizer):\n","    def make_data_strings(file_name):\n","        data_strings = []\n","        with open(os.path.join('../input/goormtextclassificationproject', file_name), 'r', encoding='utf-8') as f:\n","            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]\n","        for item in id_file_data:\n","            data_strings.append(' '.join([str(k) for k in item]))\n","        return data_strings\n","    \n","    print('it will take some times...')\n","    train_pos = make_data_strings('sentiment.train.1')\n","    train_neg = make_data_strings('sentiment.train.0')\n","    dev_pos = make_data_strings('sentiment.dev.1')\n","    dev_neg = make_data_strings('sentiment.dev.0')\n","\n","    print('make id file finished!')\n","    return train_pos, train_neg, dev_pos, dev_neg"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:31:53.790845Z","iopub.execute_input":"2021-10-31T12:31:53.791166Z","iopub.status.idle":"2021-10-31T12:31:53.799973Z","shell.execute_reply.started":"2021-10-31T12:31:53.791107Z","shell.execute_reply":"2021-10-31T12:31:53.798919Z"},"trusted":true,"id":"8L9dcp8HAVME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:31:53.803187Z","iopub.execute_input":"2021-10-31T12:31:53.803797Z","iopub.status.idle":"2021-10-31T12:31:58.176424Z","shell.execute_reply.started":"2021-10-31T12:31:53.803707Z","shell.execute_reply":"2021-10-31T12:31:58.17547Z"},"trusted":true,"id":"JJ6jyFLJAVME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:31:58.177831Z","iopub.execute_input":"2021-10-31T12:31:58.178707Z","iopub.status.idle":"2021-10-31T12:35:19.301794Z","shell.execute_reply.started":"2021-10-31T12:31:58.178663Z","shell.execute_reply":"2021-10-31T12:35:19.300759Z"},"trusted":true,"id":"NlBf0MQhAVME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_pos[:10]"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:19.303803Z","iopub.execute_input":"2021-10-31T12:35:19.304509Z","iopub.status.idle":"2021-10-31T12:35:19.315813Z","shell.execute_reply.started":"2021-10-31T12:35:19.304464Z","shell.execute_reply":"2021-10-31T12:35:19.314778Z"},"trusted":true,"id":"7gFrq-qEAVMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SentimentDataset(object):\n","    def __init__(self, tokenizer, pos, neg):\n","        self.tokenizer = tokenizer\n","        self.data = []\n","        self.label = []\n","\n","        for pos_sent in pos:\n","            self.data += [self._cast_to_int(pos_sent.strip().split())]\n","            self.label += [[1]]\n","        for neg_sent in neg:\n","            self.data += [self._cast_to_int(neg_sent.strip().split())]\n","            self.label += [[0]]\n","\n","    def _cast_to_int(self, sample):\n","        return [int(word_id) for word_id in sample]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        sample = self.data[index]\n","        return np.array(sample), np.array(self.label[index])"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:19.317729Z","iopub.execute_input":"2021-10-31T12:35:19.318489Z","iopub.status.idle":"2021-10-31T12:35:19.332009Z","shell.execute_reply.started":"2021-10-31T12:35:19.31843Z","shell.execute_reply":"2021-10-31T12:35:19.330786Z"},"trusted":true,"id":"Ggmc5EYEAVMF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\n","dev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:19.334295Z","iopub.execute_input":"2021-10-31T12:35:19.334681Z","iopub.status.idle":"2021-10-31T12:35:22.727085Z","shell.execute_reply.started":"2021-10-31T12:35:19.334621Z","shell.execute_reply":"2021-10-31T12:35:22.726165Z"},"trusted":true,"id":"j7Ey1hrRAVMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, item in enumerate(train_dataset):\n","    print(item)\n","    if i == 10:\n","        break"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:22.72867Z","iopub.execute_input":"2021-10-31T12:35:22.729001Z","iopub.status.idle":"2021-10-31T12:35:22.743743Z","shell.execute_reply.started":"2021-10-31T12:35:22.728956Z","shell.execute_reply":"2021-10-31T12:35:22.742687Z"},"trusted":true,"id":"t-CXaoJsAVMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_sentiment(samples):\n","    input_ids, labels = zip(*samples)\n","    max_len = max(len(input_id) for input_id in input_ids)\n","    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n","\n","    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n","                             batch_first=True)\n","    attention_mask = torch.tensor(\n","        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n","         sorted_indices])\n","    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n","    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n","    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n","\n","    return input_ids, attention_mask, token_type_ids, position_ids, labels"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:22.748569Z","iopub.execute_input":"2021-10-31T12:35:22.749577Z","iopub.status.idle":"2021-10-31T12:35:22.762556Z","shell.execute_reply.started":"2021-10-31T12:35:22.749531Z","shell.execute_reply":"2021-10-31T12:35:22.761374Z"},"trusted":true,"id":"fo6aDKylAVMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_batch_size=64\n","eval_batch_size=64\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=train_batch_size,\n","                                           shuffle=True, collate_fn=collate_fn_sentiment,\n","                                           pin_memory=True, num_workers=4)\n","dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n","                                         shuffle=False, collate_fn=collate_fn_sentiment,\n","                                         num_workers=2)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:22.767633Z","iopub.execute_input":"2021-10-31T12:35:22.76831Z","iopub.status.idle":"2021-10-31T12:35:22.780633Z","shell.execute_reply.started":"2021-10-31T12:35:22.768264Z","shell.execute_reply":"2021-10-31T12:35:22.779201Z"},"trusted":true,"id":"nCSjRBZfAVMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random seed\n","random_seed=42\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model1 = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model1.to(device)"],"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-31T12:35:22.783345Z","iopub.execute_input":"2021-10-31T12:35:22.783908Z","iopub.status.idle":"2021-10-31T12:35:47.828733Z","shell.execute_reply.started":"2021-10-31T12:35:22.783858Z","shell.execute_reply":"2021-10-31T12:35:47.827738Z"},"trusted":true,"id":"Wr7UiE6qAVMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","model1.train()\n","learning_rate = 5e-5\n","optimizer = Adafactor(model.parameters(),scale_parameter=False,\n","                      relative_step=False, warmup_init=False, lr=learning_rate)\n","\n","# LambdaLR은 가장 유연한 Scheduler이기 때문에 사용\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,lr_lambda=lambda epoch: 0.95 ** epoch,\n","                                              last_epoch=-1,verbose=False)\n"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:47.831459Z","iopub.execute_input":"2021-10-31T12:35:47.832056Z","iopub.status.idle":"2021-10-31T12:35:47.843362Z","shell.execute_reply.started":"2021-10-31T12:35:47.832009Z","shell.execute_reply":"2021-10-31T12:35:47.84088Z"},"trusted":true,"id":"w23IXiWpAVMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_acc(predictions, target_labels):\n","    return (np.array(predictions) == np.array(target_labels)).mean()\n","# precision = 모델이 infer 한거 중에 맞춘거\n","# racall = 정답 중에 맞춘거"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:47.845085Z","iopub.execute_input":"2021-10-31T12:35:47.84555Z","iopub.status.idle":"2021-10-31T12:35:47.88626Z","shell.execute_reply.started":"2021-10-31T12:35:47.845439Z","shell.execute_reply":"2021-10-31T12:35:47.884786Z"},"trusted":true,"id":"K5gVpgqvAVMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_epoch = 1\n","lowest_valid_loss = 9999.\n","for epoch in range(train_epoch):\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            token_type_ids = token_type_ids.to(device)\n","            position_ids = position_ids.to(device)\n","            labels = labels.to(device, dtype=torch.long)\n","\n","            optimizer.zero_grad()\n","\n","            output = model1(input_ids=input_ids,\n","                           attention_mask=attention_mask,\n","                           token_type_ids=token_type_ids,\n","                           position_ids=position_ids,\n","                           labels=labels)\n","\n","            loss = output.loss\n","            loss.backward()\n","\n","            optimizer.step()                    \n","\n","            tepoch.set_postfix(loss=loss.item())\n","            if iteration != 0 and iteration % int(len(train_loader) / 5) == 0:\n","                # Evaluate the model five times per epoch\n","                with torch.no_grad():\n","                    model1.eval()\n","                    valid_losses = []\n","                    predictions = []\n","                    target_labels = []\n","                    for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n","                                                                                                desc='Eval',\n","                                                                                                position=1,\n","                                                                                                leave=None):\n","                        input_ids = input_ids.to(device)\n","                        attention_mask = attention_mask.to(device)\n","                        token_type_ids = token_type_ids.to(device)\n","                        position_ids = position_ids.to(device)\n","                        labels = labels.to(device, dtype=torch.long)\n","\n","                        output = model(input_ids=input_ids,\n","                                       attention_mask=attention_mask,\n","                                       token_type_ids=token_type_ids,\n","                                       position_ids=position_ids,\n","                                       labels=labels)\n","\n","                        logits = output.logits\n","                        loss = output.loss\n","                        valid_losses.append(loss.item())\n","\n","                        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n","                        batch_labels = [int(example) for example in labels]\n","\n","                        predictions += batch_predictions\n","                        target_labels += batch_labels\n","\n","                acc = compute_acc(predictions, target_labels)\n","                valid_loss = sum(valid_losses) / len(valid_losses)\n","                if lowest_valid_loss > valid_loss:\n","                    # optimizer.step() 바로 아래보다 이 위치에 추가할때 더 좋은 성능이 나옴\n","                    scheduler.step() \n","                    print('Acc for model which have lower valid loss: ', acc)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T12:35:47.888418Z","iopub.execute_input":"2021-10-31T12:35:47.889215Z","iopub.status.idle":"2021-10-31T13:00:47.565026Z","shell.execute_reply.started":"2021-10-31T12:35:47.889078Z","shell.execute_reply":"2021-10-31T13:00:47.563836Z"},"trusted":true,"id":"OWrSoKsxAVMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","test_df = pd.read_csv('../input/goormtextclassificationproject/test_no_label.csv')"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:47.566931Z","iopub.execute_input":"2021-10-31T13:00:47.567674Z","iopub.status.idle":"2021-10-31T13:00:47.596058Z","shell.execute_reply.started":"2021-10-31T13:00:47.567626Z","shell.execute_reply":"2021-10-31T13:00:47.595067Z"},"trusted":true,"id":"q7f17DXwAVMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = test_df['Id']"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:47.600033Z","iopub.execute_input":"2021-10-31T13:00:47.600281Z","iopub.status.idle":"2021-10-31T13:00:47.610599Z","shell.execute_reply.started":"2021-10-31T13:00:47.600252Z","shell.execute_reply":"2021-10-31T13:00:47.609421Z"},"trusted":true,"id":"FEywxXoEAVMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_id_file_test(tokenizer, test_dataset):\n","    data_strings = []\n","    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]\n","    for item in id_file_data:\n","        data_strings.append(' '.join([str(k) for k in item]))\n","    return data_strings"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:47.612716Z","iopub.execute_input":"2021-10-31T13:00:47.613541Z","iopub.status.idle":"2021-10-31T13:00:47.622818Z","shell.execute_reply.started":"2021-10-31T13:00:47.613495Z","shell.execute_reply":"2021-10-31T13:00:47.621692Z"},"trusted":true,"id":"0poPlAcOAVMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = make_id_file_test(tokenizer, test_dataset)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:47.624327Z","iopub.execute_input":"2021-10-31T13:00:47.62501Z","iopub.status.idle":"2021-10-31T13:00:48.11644Z","shell.execute_reply.started":"2021-10-31T13:00:47.624947Z","shell.execute_reply":"2021-10-31T13:00:48.115413Z"},"trusted":true,"id":"yamAMxm-AVMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test[:10]"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:48.118043Z","iopub.execute_input":"2021-10-31T13:00:48.118367Z","iopub.status.idle":"2021-10-31T13:00:48.128372Z","shell.execute_reply.started":"2021-10-31T13:00:48.118325Z","shell.execute_reply":"2021-10-31T13:00:48.127252Z"},"trusted":true,"id":"N0OAS2UyAVMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SentimentTestDataset(object):\n","    def __init__(self, tokenizer, test):\n","        self.tokenizer = tokenizer\n","        self.data = []\n","\n","        for sent in test:\n","            self.data += [self._cast_to_int(sent.strip().split())]\n","\n","    def _cast_to_int(self, sample):\n","        return [int(word_id) for word_id in sample]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        sample = self.data[index]\n","        return np.array(sample)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:48.130375Z","iopub.execute_input":"2021-10-31T13:00:48.130709Z","iopub.status.idle":"2021-10-31T13:00:48.140413Z","shell.execute_reply.started":"2021-10-31T13:00:48.130665Z","shell.execute_reply":"2021-10-31T13:00:48.139193Z"},"trusted":true,"id":"LTm8lxceAVMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = SentimentTestDataset(tokenizer, test)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:48.143106Z","iopub.execute_input":"2021-10-31T13:00:48.143851Z","iopub.status.idle":"2021-10-31T13:00:48.158793Z","shell.execute_reply.started":"2021-10-31T13:00:48.143789Z","shell.execute_reply":"2021-10-31T13:00:48.157668Z"},"trusted":true,"id":"PFpxuSM4AVML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_sentiment_test(samples):\n","    input_ids = samples\n","    max_len = max(len(input_id) for input_id in input_ids)\n","    #sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n","    sorted_indices = list(range(len(samples)))\n","\n","    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n","                             batch_first=True)\n","    attention_mask = torch.tensor(\n","        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n","         sorted_indices])\n","    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n","    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n","\n","    return input_ids, attention_mask, token_type_ids, position_ids"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:48.160761Z","iopub.execute_input":"2021-10-31T13:00:48.161231Z","iopub.status.idle":"2021-10-31T13:00:48.172941Z","shell.execute_reply.started":"2021-10-31T13:00:48.161186Z","shell.execute_reply":"2021-10-31T13:00:48.171584Z"},"trusted":true,"id":"YlD_dW5ZAVML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_batch_size = 32\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n","                                          shuffle=False, collate_fn=collate_fn_sentiment_test,\n","                                          num_workers=2)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:48.174347Z","iopub.execute_input":"2021-10-31T13:00:48.175011Z","iopub.status.idle":"2021-10-31T13:00:48.184308Z","shell.execute_reply.started":"2021-10-31T13:00:48.174958Z","shell.execute_reply":"2021-10-31T13:00:48.183018Z"},"trusted":true,"id":"PQSXm0jPAVML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    model1.eval()\n","    predictions1 = []\n","    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n","                                                                        desc='Test',\n","                                                                        position=1,\n","                                                                        leave=None):\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        token_type_ids = token_type_ids.to(device)\n","        position_ids = position_ids.to(device)\n","\n","        output = model1(input_ids=input_ids,\n","                       attention_mask=attention_mask,\n","                       token_type_ids=token_type_ids,\n","                       position_ids=position_ids)\n","\n","        logits = output.logits\n","        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n","        predictions1 += batch_predictions"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:48.187197Z","iopub.execute_input":"2021-10-31T13:00:48.187879Z","iopub.status.idle":"2021-10-31T13:00:49.324541Z","shell.execute_reply.started":"2021-10-31T13:00:48.187819Z","shell.execute_reply":"2021-10-31T13:00:49.32352Z"},"trusted":true,"id":"Xu3HPDCbAVML"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model2"],"metadata":{"id":"NYtF7BtSAVML"}},{"cell_type":"code","source":["train_batch_size=64\n","eval_batch_size=64\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=train_batch_size,\n","                                           shuffle=True, collate_fn=collate_fn_sentiment,\n","                                           pin_memory=True, num_workers=4)\n","dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n","                                         shuffle=False, collate_fn=collate_fn_sentiment,\n","                                         num_workers=2)"],"metadata":{"id":"BI_4MDI_AVMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random seed\n","random_seed=42\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model2 = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"metadata":{"id":"UQVgAtSWAVMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2.train()\n","learning_rate = 5e-5\n","optimizer = AdamW(model2.parameters(), lr=learning_rate)"],"metadata":{"id":"zR8aFBp9AVMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_epoch = 1\n","lowest_valid_loss = 9999.\n","for epoch in range(train_epoch):\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            token_type_ids = token_type_ids.to(device)\n","            position_ids = position_ids.to(device)\n","            labels = labels.to(device, dtype=torch.long)\n","\n","            optimizer.zero_grad()\n","\n","            output = model2(input_ids=input_ids,\n","                           attention_mask=attention_mask,\n","                           token_type_ids=token_type_ids,\n","                           position_ids=position_ids,\n","                           labels=labels)\n","\n","            loss = output.loss\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            tepoch.set_postfix(loss=loss.item())\n","            if iteration != 0 and iteration % int(len(train_loader) / 5) == 0:\n","                # Evaluate the model five times per epoch\n","                with torch.no_grad():\n","                    model2.eval()\n","                    valid_losses = []\n","                    predictions = []\n","                    target_labels = []\n","                    for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n","                                                                                                desc='Eval',\n","                                                                                                position=1,\n","                                                                                                leave=None):\n","                        input_ids = input_ids.to(device)\n","                        attention_mask = attention_mask.to(device)\n","                        token_type_ids = token_type_ids.to(device)\n","                        position_ids = position_ids.to(device)\n","                        labels = labels.to(device, dtype=torch.long)\n","\n","                        output = model2(input_ids=input_ids,\n","                                       attention_mask=attention_mask,\n","                                       token_type_ids=token_type_ids,\n","                                       position_ids=position_ids,\n","                                       labels=labels)\n","\n","                        logits = output.logits\n","                        loss = output.loss\n","                        valid_losses.append(loss.item())\n","\n","                        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n","                        batch_labels = [int(example) for example in labels]\n","\n","                        predictions += batch_predictions\n","                        target_labels += batch_labels\n","\n","                acc = compute_acc(predictions, target_labels)\n","                valid_loss = sum(valid_losses) / len(valid_losses)\n","                if lowest_valid_loss > valid_loss:\n","                    print('Acc for model which have lower valid loss: ', acc)"],"metadata":{"id":"GwY7ALWtAVMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_batch_size = 32\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n","                                          shuffle=False, collate_fn=collate_fn_sentiment_test,\n","                                          num_workers=2)"],"metadata":{"id":"gOEYVUl2AVMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    model2.eval()\n","    predictions2 = []\n","    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n","                                                                        desc='Test',\n","                                                                        position=1,\n","                                                                        leave=None):\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        token_type_ids = token_type_ids.to(device)\n","        position_ids = position_ids.to(device)\n","\n","        output = model2(input_ids=input_ids,\n","                       attention_mask=attention_mask,\n","                       token_type_ids=token_type_ids,\n","                       position_ids=position_ids)\n","\n","        logits = output.logits\n","        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n","        predictions2 += batch_predictions"],"metadata":{"id":"Ukov-aptAVMM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model3"],"metadata":{"id":"jDSQ_EtwAVMN"}},{"cell_type":"code","source":["text_model = 'distilbert-base-uncased'\n","tokenizer = DistilBertTokenizerFast.from_pretrained(text_model)"],"metadata":{"id":"w5bRoy4eAVMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_style(samples):\n","    input_ids, labels = zip(*samples)\n","    max_len = max(len(input_id) for input_id in input_ids)\n","    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n","\n","    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n","                             batch_first=True)\n","    attention_mask = torch.tensor(\n","        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n","         sorted_indices])\n","    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n","    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n","    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n","\n","    return input_ids, attention_mask, token_type_ids, position_ids, labels"],"metadata":{"id":"y27kajPhAVMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_batch_size=128\n","eval_batch_size=128\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=train_batch_size,\n","                                           shuffle=True, collate_fn=collate_fn_style,\n","                                           pin_memory=True, num_workers=4)\n","dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n","                                         shuffle=False, collate_fn=collate_fn_style,\n","                                         num_workers=2)"],"metadata":{"id":"MGVY8hU1AVMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# random seed\n","random_seed=42\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model3 = DistilBertForSequenceClassification.from_pretrained(text_model)\n","model3.to(device)"],"metadata":{"id":"GRG17VLKAVMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model3.train()\n","learning_rate = 2e-5\n","optimizer = Adafactor(model1.parameters(),scale_parameter=False,\n","                      relative_step=False, warmup_init=False, lr=learning_rate)\n","\n","# LambdaLR은 가장 유연한 Scheduler이기 때문에 사용\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,lr_lambda=lambda epoch: 0.95 ** epoch,\n","                                              last_epoch=-1,verbose=False)"],"metadata":{"id":"Dof-IaRcAVMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_epoch = 4\n","lowest_valid_loss = 9999.\n","min_loss = 9999.\n","max_acc = 0.\n","for epoch in range(train_epoch):\n","    with tqdm(train_loader, unit=\"batch\") as tepoch:\n","        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            token_type_ids = token_type_ids.to(device)\n","            position_ids = position_ids.to(device)\n","            labels = labels.to(device, dtype=torch.long)\n","\n","            optimizer.zero_grad()\n","\n","            output = model3(input_ids=input_ids,\n","                           attention_mask=attention_mask,\n","                           # token_type_ids=token_type_ids,\n","                           #position_ids=position_ids,\n","                           labels=labels)\n","\n","            loss = output.loss\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            tepoch.set_postfix(loss=loss.item())\n","            if iteration != 0 and iteration % int(len(train_loader) / 20) == 0:\n","                # Evaluate the model five times per epoch\n","                with torch.no_grad():\n","                    model3.eval()\n","                    valid_losses = []\n","                    predictions = []\n","                    target_labels = []\n","                    for input_ids, attention_mask, token_type_ids, position_ids, labels in dev_loader:\n","#                     for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n","#                                                                                                 desc='Eval',\n","#                                                                                                 position=1,\n","#                                                                                                 leave=None):\n","                        input_ids = input_ids.to(device)\n","                        attention_mask = attention_mask.to(device)\n","                        token_type_ids = token_type_ids.to(device)\n","                        position_ids = position_ids.to(device)\n","                        labels = labels.to(device, dtype=torch.long)\n","\n","                        output = model3(input_ids=input_ids,\n","                                       attention_mask=attention_mask,\n","                                       #token_type_ids=token_type_ids,\n","                                       # position_ids=position_ids,\n","                                       labels=labels)\n","\n","                        logits = output.logits\n","                        loss = output.loss\n","                        valid_losses.append(loss.item())\n","\n","                        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n","                        batch_labels = [int(example) for example in labels]\n","\n","                        predictions += batch_predictions\n","                        target_labels += batch_labels\n","\n","                acc = compute_acc(predictions, target_labels)\n","                valid_loss = sum(valid_losses) / len(valid_losses)\n","                \n","                if max_acc < acc:\n","                    min_loss = valid_loss\n","                    max_acc = acc\n","                    torch.save({\n","                        'epoch': epoch,\n","                        'iteration': iteration,\n","                        'model_state_dict': model3.state_dict(),\n","                        'optimzier_state_dict': optimizer.state_dict(),\n","                        'valid_loss': valid_loss,\n","                        'acc' : max_acc,\n","                        }, './model.pt')\n","                if lowest_valid_loss > valid_loss:\n","                    # optimizer.step() 바로 아래보다 이 위치에 추가할때 더 좋은 성능이 나옴\n","                    scheduler.step() \n","                    print('Acc for model which have lower valid loss: ', acc, max_acc)"],"metadata":{"id":"kcHicgcKAVMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn_style_test(samples):\n","    input_ids = samples\n","    max_len = max(len(input_id) for input_id in input_ids)\n","    # sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n","    sorted_indices = list(range(len(samples)))\n","    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n","                             batch_first=True)\n","    attention_mask = torch.tensor(\n","        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n","         sorted_indices])\n","    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n","    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n","\n","    return input_ids, attention_mask, token_type_ids, position_ids"],"metadata":{"id":"ytQ3TV-fAVMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_batch_size = 128\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n","                                          shuffle=False, collate_fn=collate_fn_style_test,\n","                                          num_workers=2)"],"metadata":{"id":"fySU2Nr9AVMO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(test_dataset)\n","checkpoint = torch.load('./model.pt')\n","model3.load_state_dict(checkpoint['model_state_dict'])"],"metadata":{"id":"_RwYbg3rAVMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    model3.eval()\n","    predictions3 = []\n","    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n","                                                                        desc='Test',\n","                                                                        position=1,\n","                                                                        leave=None):\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        token_type_ids = token_type_ids.to(device)\n","        position_ids = position_ids.to(device)\n","\n","        output = model3(input_ids=input_ids,\n","                       attention_mask=attention_mask)\n","#         ,\n","#                        token_type_ids=token_type_ids,\n","#                        position_ids=position_ids)\n","                       #labels=labels)\n","\n","        logits = output.logits\n","        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n","        predictions3 += batch_predictions"],"metadata":{"id":"Kbwe4SNtAVMP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hard-Voting Ensemble"],"metadata":{"id":"GjK9vX7GAVMP"}},{"cell_type":"code","source":["a = b = c = n = 0\n","predictions = []\n","for i in range (len(predictions1)):\n","    if predictions1[i] + predictions2[i] + predictions3[i] == 3:\n","        a += 1\n","    elif predictions1[i] + predictions2[i] + predictions3[i] == 2:\n","        b += 1\n","        print (i)\n","    elif predictions1[i] + predictions2[i] + predictions3[i] == 1:\n","        c += 1\n","        print (i)\n","    else:\n","        n += 1\n","        \n","    if (predictions1[i] == predictions2[i]) and (predictions2[i] != predictions3[i]):\n","        predictions.append(predictions1[i])\n","    else:\n","        predictions.append(int(round((predictions1[i] + predictions2[i] + predictions3[i])/3,0)))"],"metadata":{"id":"mCYMEJ3bAVMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print (a, b, c, n, a+ b+ c+ n)"],"metadata":{"id":"Ca1Y0B0_AVMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions"],"metadata":{"id":"RB0HEiPNAVMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df['Category'] = predictions"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:49.32693Z","iopub.execute_input":"2021-10-31T13:00:49.327333Z","iopub.status.idle":"2021-10-31T13:00:49.336306Z","shell.execute_reply.started":"2021-10-31T13:00:49.327282Z","shell.execute_reply":"2021-10-31T13:00:49.335227Z"},"trusted":true,"id":"7YhANoxfAVMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2021-10-31T13:00:49.343032Z","iopub.execute_input":"2021-10-31T13:00:49.343814Z","iopub.status.idle":"2021-10-31T13:00:49.357699Z","shell.execute_reply.started":"2021-10-31T13:00:49.34378Z","shell.execute_reply":"2021-10-31T13:00:49.356764Z"},"trusted":true,"id":"cflPepvMAVMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"i9DWmfzdAVMP"},"execution_count":null,"outputs":[]}]}